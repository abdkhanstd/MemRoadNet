## MemRoadNet
Code for our paper "MemRoadNet: Human-Like Memory Integration for Free Road Space Detection"

# InternImage-XL + UperNet with Human-like Memory for Road Segmentation

A PyTorch implementation of **InternImage-XL backbone** combined with **UperNet decoder** and an **enhanced human-like memory system** for real-time road segmentation.

## ğŸ“ Project Structure

```
upload/
â”œâ”€â”€ models/                     # Model implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ internimage.py         # InternImage-XL backbone
â”‚   â”œâ”€â”€ upernet.py             # UperNet decoder
â”‚   â”œâ”€â”€ memory_system.py       # Human-like memory bank
â”‚   â””â”€â”€ memory_augmented_model.py  # Complete model
â”œâ”€â”€ data/                      # Dataset utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py            # Dataset class and utilities
â”œâ”€â”€ utils/                     # Training utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ training_utils.py     # Loss, metrics, and utilities
â”œâ”€â”€ config.py                 # Configuration settings
â”œâ”€â”€ train.py                  # Training script
â”œâ”€â”€ inference.py              # Inference script
â””â”€â”€ requirements.txt          # Dependencies
```

## ğŸ› ï¸ Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/abdkhanstd/MemRoadNet
   cd upload
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Download pretrained weights**:
   - Download InternImage-XL pretrained weights to `pretrained/upernet_internimage_xl_640_160k_ade20k.pth`
   - Alternatively, update `config.py` with your pretrained weights path

## ğŸ“Š Dataset Structure

Organize your dataset as follows:
```
datasets/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”‚   â””â”€â”€ image2.jpg
â”‚   â””â”€â”€ masks/
â”‚       â”œâ”€â”€ image1.png
â”‚       â””â”€â”€ image2.png
â””â”€â”€ val/
    â”œâ”€â”€ images/
    â””â”€â”€ masks/
```

## ğŸš€ Training

1. **Configure settings** in `config.py`:
   ```python
   # Model Configuration
   BATCH_SIZE = 4
   LEARNING_RATE = 1e-4
   EPOCHS = 100
   
   # Memory System
   MEMORY_SIZE = 1000
   MEMORY_WEIGHT = 0.3
   USE_MEMORY_IN_TRAINING = True
   ```

2. **Start training**:
   ```bash
   python train.py
   ```

3. **Monitor progress**:
   - Training logs with memory statistics
   - Validation samples saved to `val_samples_intern/`
   - Best model saved to `weights_simple/best_internimage_upernet_with_memory.pth`

## ğŸ” Inference

1. **Prepare test images** in `test_images/` directory

2. **Run inference**:
   ```bash
   python inference.py
   ```

3. **Results saved to**:
   - `inference_results/masks/` - Segmentation masks
   - `inference_results/inference_summary.txt` - Performance summary

## ğŸ§  Memory System

The human-like memory system includes:

- **Episodic Memory**: Specific experiences with rich context
- **Semantic Memory**: Generalized knowledge patterns  
- **Working Memory**: Recent short-term context
- **Emotional Valence**: Success/failure emotions guide consolidation
- **Forgetting Mechanism**: Gradual decay with importance-based retention
- **Consolidation**: Sleep-like memory strengthening between epochs

## âš™ï¸ Configuration

Key configuration options in `config.py`:

```python
class Config:
    # Model Architecture
    CHANNELS = 192
    DEPTHS = [5, 5, 24, 5]
    GROUPS = [12, 24, 48, 96]
    
    # Training
    BATCH_SIZE = 4
    LEARNING_RATE = 1e-4
    EPOCHS = 100
    
    # Memory System
    MEMORY_SIZE = 1000
    TOP_K_MEMORIES = 5
    MEMORY_WEIGHT = 0.3
    MEMORY_CONSOLIDATION_INTERVAL = 5
    
    # Paths
    DATASET_DIR = "datasets"
    WEIGHTS_PATH = "./weights_simple"
    PRETRAINED_PATH = "pretrained/upernet_internimage_xl_640_160k_ade20k.pth"
```


## ğŸ“ Citation

```bibtex

```


## ğŸ™ Acknowledgments

- InternImage team for the excellent backbone architecture
- UperNet authors for the segmentation head design
- PyTorch team for the deep learning framework
